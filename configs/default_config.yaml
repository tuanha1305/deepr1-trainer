model:
  input_dim: 128
  hidden_dim: 256
  output_dim: 128
  num_layers: 2
  use_hyperbolic: true
  hyperbolic_c: 1.0
  dropout: 0.1
  model_type: "SmallRLModel"

training:
  learning_rate: 1e-4
  batch_size: 32
  epochs: 10
  max_seq_length: 512
  optimizer: "RiemannianAdam"
  gamma: 0.99
  epsilon_start: 1.0
  epsilon_end: 0.01
  epsilon_decay: 1000
  accuracy_reward_weight: 1.0
  format_reward_weight: 0.5

data:
  train_data_path: "data/train"
  eval_data_path: "data/eval"
  test_data_path: "data/test"
  max_seq_length: 512
  vocab_size: 50000
  tokenizer_name: "gpt2"
  dataset_name: "custom"
  num_workers: 4
  use_augmentation: false